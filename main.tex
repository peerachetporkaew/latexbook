\documentclass[17pt,fancychapters]{report}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage{listings}
\usepackage{color}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{subcaption}
\usepackage{cancel}
\usepackage{tikz}
\usepackage{color}
\usepackage{titlesec}
\usepackage{hyperref}
\usepackage{fouriernc}

\titleformat{\chapter}
{\normalfont\LARGE\bfseries}{บทที่ \thechapter}{1em}{}

\titleformat{\section}
{\normalfont\bfseries}{\thesection}{1em}{}

\renewcommand{\figurename}{ภาพที่ }
\renewcommand{\contentsname}{สารบัญ }

\usepackage{fontspec}
\usepackage{xltxtra}
\XeTeXlinebreaklocale "th_TH"
\XeTeXlinebreakskip = 0pt plus 1pt  
\usepackage{fonts-tlwg}

\defaultfontfeatures{Scale=1.6}



\setmainfont[
BoldFont={THSarabunNewBold.ttf},
ItalicFont={THSarabunNewItalic.ttf},
BoldItalicFont={THSarabunNewBoldItalic.ttf}
]{THSarabunNew.ttf}

\linespread{1.3}

\usetikzlibrary{calc,trees,positioning,arrows,chains,shapes.geometric,%
    decorations.pathreplacing,decorations.pathmorphing,shapes,%
    matrix,shapes.symbols}

\geometry{top=1.3in,bottom=1.3in}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\SetSymbolFont{letters}{normal}{T1}{cmr}{m}{sl}
\SetSymbolFont{letters}{bold}  {T1}{cmr}{b}{sl}

\definecolor{DarkerGreen}{RGB}{0,179,45}

\include{python_highlighting}

\newtheorem{exmp}{Example}[section]

\newcommand{\codeExample}[2]{
	\begin{exmp}
      #1
      \noindent\begin{minipage}{\linewidth}
      \begin{lstlisting}[style=python]
          #2
      \end{lstlisting}
      \end{minipage}
    \end{exmp}
}

\newcommand\MyLBrace[2]{%
  \left.\rule{0pt}{#1}\right\}\text{#2}}
  
\tikzset{
>=stealth',
  punktchain/.style={
    rectangle, 
    rounded corners, 
    % fill=black!10,
    draw=black, very thick,
    text width=10em, 
    minimum height=3em, 
    text centered, 
    on chain},
  line/.style={draw, thick, <-},
  element/.style={
    tape,
    top color=white,
    bottom color=blue!50!black!60!,
    minimum width=8em,
    draw=blue!40!black!90, very thick,
    text width=10em, 
    minimum height=3.5em, 
    text centered, 
    on chain},
  every join/.style={->, thick,shorten >=1pt},
  decoration={brace},
  tuborg/.style={decorate},
  tubnode/.style={midway, right=2pt},
}

\title{ระบบแปลภาษาอัตโนมัติด้วยโครงข่ายประสาทเทียม}
\author{พีรเชษฐ  ปอแก้ว}
\date{December 30, 2015}

\begin{document}

\maketitle
\pagenumbering{gobble}
\newpage
\pagenumbering{roman}
\tableofcontents
\newpage
\pagenumbering{arabic}

\chapter{Neural Machine Translation By Jointly \\ Learning to Align and Translate}

\textbf{บทคัดย่อ}

Neural Machine Translation (ระบบแปลภาษาอัตโนมัติด้วยโครงข่ายประสาทเทียม) เป็นวิธีการแปลภาษาอัตโนมัติรูปแบบใหม่ ต่างจากระบบแปลภาษาเชิงสถิติ กล่าวคือ ระบบนี้สามารถปรับจูนพารามิเตอร์ของระบบ (ที่มีเพียงหนึ่งองค์ประกอบ) เพื่อเพิ่มประสิทธิภาพในการแปลได้ ระบบแปลชนิดนีโดยมากอาศัย หลักการของ encoder-decoder ตัว encoder ทำหน้าที่อ่านประโยคต้นทางและบีบอัดให้กลายเป็นเวกเตอร์ที่มีความยาวจำกัด และส่งต่อให้ decoder ทำหน้าที่สร้างประโยคในภาษาปลายทาง ในงานวิจัยนี้เรามีความเห็นว่า การใช้เวกเตอร์ที่มีความยาวจำกัดมาเก็บประโยคทั้งประโยคนั้นเป็นคอขวดของระบบ ที่ทำให้การเพิ่มประสิทธิภาพของการแปลด้วยวิธีนี้ทำได้ยาก เราจึงได้นำเสนอวิธีการใหม่ที่ยอมให้โมเดลสามารถค้นหาส่วนของประโยคที่สัมพันธ์กับคำแปลปัจจุบันได้ โดยไม่จำเป็นต้องผ่านการตัดประโยคออกเป็นส่วนๆ วิธีการนี้ทำให้เราได้ผลลัพธ์ที่ดีเทียบเท่ากับระบบแปลภาษาเชิงสถิติของภาษาอังกฤษเป็นฝรั่งเศส นอกจากนี้ ผลการวิเคราะห์คุณภาพยังแสดงให้เห็นว่า alignment ที่ได้จากระบบนั้นเป็นไปตามสิ่งที่เราคาดหวังไว้ 


\section{บทนำ}
Neural Machine Translation (NMT) เป็นวิธีการแปลภาษาอัตโนมัติรูปแบบใหม่ ซึ่งนำเสนอโดย Kalchbrenner and Blunsom (2013), Sutskever et al. (2014) and Cho et al. (2014b). วิธีการนี้แตกต่างจากระบบแปลภาษาเชิงสถิติ (ดูตัวอย่างเช่น Koehn et al., 2003) ที่จำเป็นต้องสร้างส่วนประกอบย่อยๆ ขึ้นมาแล้วจึงปรับพารามิเตอร์รวมเข้าด้วยกัน วิธีการของ NMT นั้นพยายามสร้างและฝึกสอนระบบโครงข่ายประสาทเทียมที่ต่อกันเป็นระบบเดียว เพื่ออ่านประโยคและแปลเป็นประโยคที่ถูกต้อง

ระบบแปลภาษาด้วยโครงข่ายประสาทเทียมโดยมากนั้นถูกจัดอยู่ในตระกูลของ encoder-decoder  (Sutskever et al., 2014; Cho et al., 2014a) ซึ่งทำหน้าที่จัดการกับภาษาต้นทางและภาษาปลายทางตามลำดับ หรืออาจจะมีส่วนประกอบอื่นเพื่อจัดการกับลักษณะเฉพาะของภาษา หรือการรวมกันของผลลัพธ์ (Hermann and Blunsom, 2014). ตัว encoder ทำหน้าที่อ่านประโยคต้นทางและบีบอัดให้กลายเป็นเวกเตอร์ที่มีความยาวจำกัด ส่วน decoder ทำหน้าที่สร้างประโยคในภาษาปลายทางจากเวกเตอร์ที่ได้นั้น ทั้ง encoder และ decoder ถูกฝึกสอนด้วยคลังตัวอย่างคู่ประโยคเพื่อให้ความน่าจะเป็นของคำแปลที่ถูกต้องมีค่าสูงสุด

ประสิทธิภาพของตัว encoder-decoder นี้อยู่ที่ตัวโครงข่ายประสาทเทียมที่สามารถเก็บเอาข้อมูลที่สำคัญของประโยคไว้ในตัวมันเอง (ซึ่งเป็นเวกเตอร์ขนาดจำกัด) ซึ่งมีข้อจำกัดที่ว่า เมื่อประโยคต้นทางมีความยาวเพิ่มขึ้น ความสามารถในการจดจำข้อมูลลงในเวกเตอร์ก็จะยิ่งลดลง โดยเฉพาะเมื่อความยาวประโยคที่จะแปลนั้นยาวกว่าประโยคที่ใช้ในการฝึกสอน ในงานวิจัยของ Cho et al. (2014b) แสดงให้เห็นว่าประสิทธิภาพของสถาปัยกรรมแบบ encoder-decoder ลดลงอย่างเห็นได้ชัดเมื่อประโยคนำเข้ามีความยาวเพิ่มขึ้น

เพื่อที่จะจัดการกับปัญหานี้ เราได้นำเสนอส่วนขยายของสถาปัตยกรรม encoder-decoder ซึ่งสามารถเรียนรู้ที่จะจับคู่คำและแปลได้ไปพร้อมๆ กัน แต่ละช่วงเวลาของการสร้างคำแปล ตัวโมเดลจะค้นหาตำแหน่งที่เกี่ยวข้องในประโยคภาษาต้นทาง แล้วสร้างคำแปลจากเวกเตอร์ผลลัพธ์ที่สัมพันธ์กับตำแหน่งที่หามาได้นั้น

คุณสมบัติที่แตกต่างระหว่างระบบที่พัฒนาขึ้นกับสถาปัตยกรรม encoder-decoder นั้นก็คือ ระบบนี้ไม่ได้บีบอัด (หรือเข้ารหัส) ประโยคทั้งหมดลงไปที่เวกเตอร์ตัวเดียว แต่ใช้วิธีการเข้ารหัสประโยคต้นทางให้กลายเป็นลำดับของเวกเตอร์ และให้ระบบเลือกสับเซตย่อยของลำดับเวกเตอร์นี้ขึ้นมาแปล วิธีเช่นนี้ทำให้แบบจำลองเป็นอิสระจากการบีบอัดข้อมูลทั้งประโยคลงไปในเวกเตอร์ เราได้แสดงให้เห็นว่าวิธีการนี้แปลประโยคที่ยาวได้มีประสิทธิภาพกว่าวิธีการแบบเดิม

\section{ภูมิหลัง : Neural Machine Translation : NMT}

จากมุมมองของทฤษฏีความน่าจะเป็น การแปล คือ การหาประโยคในภาษาปลายทาง \scalebox{1.2}{$\mathrm{y}$}  ที่ทำให้ ความน่าจะเป็นแบบมีเงื่อนไข (conditional probability) ของประโยคปลายทาง \scalebox{1.2}{$\mathrm{y}$} เมื่อกำหนดประโยคต้นทางเป็น \scalebox{1.2}{$\mathrm{x}$} กล่าวคือ 
\scalebox{1.2}{ $ \mathrm{argmax}_y p(\mathrm{y}| \mathrm{x}) $ } ในระบบ NMT เราจะปรับพารามิเตอร์ทั้งหมดของระบบเพื่อให้ค่าความน่าจะเป็นนี้สูงสุดโดยใช้คลังข้อมูลตัวอย่างคู่ประโยค (training data) เมื่อระบบเรียนรู้ดีแล้ว เราก็สามารถใช้โมเดลนี้ในการแปลประโยคที่ป้อนให้กับระบบแปลโดยการหาประโยคผลลัพธ์ที่ให้ค่า ความน่าจะเป็นแบบมีเงื่อนไขสูงสุดเช่นกัน



\end{document}